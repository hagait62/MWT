## Multilingual Word Translation
This repo contains the source code for our paper:

[**Multilingual word translation using auxiliary languages**](https://www.aclweb.org/anthology/D19-1134.pdf)
<br>
Hagai Taitelbaum,
[Gal Chechik](https://chechiklab.biu.ac.il/~gal/),
[Jacob Goldberger](http://www.eng.biu.ac.il/goldbej/)
<br>
EMNLP 2019
<br>
[bibtex](https://www.aclweb.org/anthology/D19-1134.bib)

(built over MUSE implementation, https://github.com/facebookresearch/MUSE)

## Environment
For installing the appropriate environment with conda, run the following commands (with anaconda3):
```
git clone https://github.com/hagait62/MWT.git
cd MWT
conda create --name mwt_env python=3.6
conda activate mwt_env
conda config --append channels conda-forge
conda config --append channels pytorch
conda config --append channels anaconda
conda install --yes --file requirements.txt
```

## Main dependencies (already installed by requirements.txt)
* Python 3.6 with [NumPy](http://www.numpy.org/)/[SciPy](https://www.scipy.org/)
* [PyTorch](http://pytorch.org/)
* [Faiss](https://github.com/facebookresearch/faiss) (recommended) for fast nearest neighbor search (CPU or GPU).

Faiss is *optional* for GPU users - though Faiss-GPU will greatly speed up nearest neighbor search - and *highly recommended* for CPU users. Faiss can be installed using "conda install faiss-cpu -c pytorch" or "conda install faiss-gpu -c pytorch".

## Get monolingual word embeddings
For pre-trained monolingual word embeddings, we highly recommend [fastText Wikipedia embeddings](https://fasttext.cc/docs/en/pretrained-vectors.html), or using [fastText](https://github.com/facebookresearch/fastText) to train your own word embeddings from your corpus.

You can download the embeddings of several languages by running:
```bash
sh get_monolingual_embeddings.sh "LANGS" PATH
```
where *LANGS* is all the desired languages (space-seperated),
and *PATH* is the directory to save the embeddings in (optional, default is ./data/vecs/).
For example:
```bash
sh get_monolingual_embeddings.sh "en de fr es it pt" ./data/vecs/
```
### Word embedding format

When loading embeddings, the model can load:
* PyTorch binary files previously generated by MUSE (.pth files).
* fastText binary files previously generated by fastText (.bin files).
* text files (text file with one word embedding per line).

The two first options are very fast and can load 1 million embeddings in a few seconds, while loading text files can take a while.

## Get evaluation dictionaries
Download evaluation dictionaries for several languages:
```bash
sh get_evaluation_dictionaries.sh "LANGS" PATH
```
where *LANGS* is all the desired languages (space-seperated),
and *PATH* is the directory to save the dictionaries in (optional, default is ./data/dictionaries/).
All available permutations will be downloaded.

The evaluation datasets are also available at:
* MUSE [bilingual dictionaries](https://github.com/facebookresearch/MUSE#ground-truth-bilingual-dictionaries)

## Translate words using MWT
For evaluating MWT, simply run:
```bash
python3 evaluation.py --langs en de fr es it pt --dicts_path DICTS_PATH --mappings_path MAPPINGS_PATH --embs DATA/VECS/wiki.en.vec DATA/VECS/wiki.de.vec DATA/VECS/wiki.fr.vec DATA/VECS/wiki.es.vec DATA/VECS/wiki.it.vec DATA/VECS/wiki.pt.vec --multilingual_inference_method BI NT CNT CAT
```
*DATA/VECS/* is the directory which contains the embeddings,
*DICTS_PATH* is the directory which contains the evaluation dictionaries,
and *MAPPINGS_PATH* is the directory which contains the mappings (e.g., `./data/mappings/six_european/`).

By default we re-norm and center the embeddings. To change it use `--normalize_embeddings=`.

## Multilingual mappings
We provide the mappings used in our experiments under ./data/mappings/.
Note that all the embeddings were mapped into the English space, so English mapping is the Identity Matrix.

The mappings for the six-european experiment were extracted by running [MAT+MPSR source code](https://github.com/ccsasuke/umwe) with the following command:
```bash
python3 unsupervised.py --src_langs de fr es it pt --src_embs VECS/wiki.de.vec VECS/wiki.fr.vec VECS/wiki.es.vec VECS/wiki.it.vec VECS/wiki.pt.vec --tgt_emb VECS/wiki.en.vec --normalize_embeddings renorm,center
```
The mappings for the european-asian experiment were extracted by running [MPSR source code](https://github.com/ccsasuke/umwe) with the following command:
```bash
python3 supervised.py --src_langs de fr es it ja zh ko --src_embs VECS/wiki.de.vec VECS/wiki.fr.vec VECS/wiki.es.vec VECS/wiki.it.vec VECS/wiki.ja.vec VECS/wiki.zh.vec VECS/wiki.ko.vec --tgt_emb VECS/wiki.en.vec --normalize_embeddings renorm,center --dico_train identical_char
```
where *VECS/* is the directory which contains the embeddings.